# LLM을 사용하는 클라이언트 만들기

지금까지 서버와 클라이언트를 만드는 방법을 보았습니다. 클라이언트는 서버에 명시적으로 호출하여 도구, 리소스 및 프롬프트를 나열할 수 있었습니다. 하지만 이것은 그리 실용적인 접근법이 아닙니다. 사용자는 에이전트 시대에 살고 있으며, 프롬프트를 사용하고 LLM과 소통하기를 기대합니다. 사용자는 MCP를 사용하여 기능을 저장하는지 여부에는 신경 쓰지 않지만, 자연어를 사용하여 상호작용하기를 기대합니다. 그렇다면 어떻게 해결할까요? 해결책은 클라이언트에 LLM을 추가하는 것입니다.

## 개요

이번 수업에서는 클라이언트에 LLM을 추가하는 방법에 집중하고, 이것이 사용자에게 훨씬 더 나은 경험을 제공하는 방법을 보여줍니다.

## 학습 목표

이 수업이 끝나면 다음을 할 수 있습니다:

- LLM이 포함된 클라이언트를 생성합니다.
- LLM을 사용하여 MCP 서버와 원활하게 상호작용합니다.
- 클라이언트 측에서 더 나은 최종 사용자 경험을 제공합니다.

## 접근법

우리가 취해야 할 접근법을 이해해 봅시다. LLM을 추가하는 것은 간단해 보이지만 실제로 어떻게 할까요?

클라이언트가 서버와 상호작용하는 방법은 다음과 같습니다:

1. 서버와 연결을 설정합니다.

2. 기능, 프롬프트, 리소스 및 도구를 나열하고 해당 스키마를 저장합니다.

3. LLM을 추가하고 저장된 기능과 스키마를 LLM이 이해하는 형식으로 전달합니다.

4. 사용자 프롬프트를 LLM에 전달하고 클라이언트가 나열한 도구와 함께 처리합니다.

이제 높은 수준에서 어떻게 할 수 있는지 이해했으니 아래 연습 문제에서 시도해 봅시다.

## 연습: LLM이 포함된 클라이언트 만들기

이번 연습에서는 클라이언트에 LLM을 추가하는 방법을 배웁니다.

### GitHub 개인 액세스 토큰을 사용한 인증

GitHub 토큰 생성은 간단한 과정입니다. 방법은 다음과 같습니다:

- GitHub 설정으로 이동 – 오른쪽 상단 프로필 사진을 클릭하고 설정을 선택합니다.
- 개발자 설정으로 이동 – 아래로 스크롤하여 개발자 설정을 클릭합니다.
- 개인 액세스 토큰 선택 – 세분화된 토큰을 클릭한 후 새 토큰 생성을 클릭합니다.
- 토큰 구성 – 참조용 메모를 추가하고 만료 날짜를 설정하며 필요한 범위(권한)를 선택합니다. 이 경우 Models 권한을 반드시 추가하세요.
- 토큰 생성 및 복사 – 토큰 생성을 클릭하고 즉시 복사하세요. 다시 볼 수 없습니다.

### -1- 서버에 연결

먼저 클라이언트를 만들어 봅시다:

#### Python

```python
from mcp import ClientSession, StdioServerParameters, types
from mcp.client.stdio import stdio_client

# stdio 연결을 위한 서버 매개변수 생성
server_params = StdioServerParameters(
    command="mcp",  # 실행 파일
    args=["run", "server.py"],  # 선택적 명령줄 인수
    env=None,  # 선택적 환경 변수
)


async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(
            read, write
        ) as session:
            # 연결 초기화
            await session.initialize()


if __name__ == "__main__":
    import asyncio

    asyncio.run(run())

```

위 코드에서 우리는:

- MCP에 필요한 라이브러리를 가져왔습니다.
- 클라이언트를 생성했습니다.
다음 단계로 서버의 기능을 나열해 봅시다.

### -2- 서버 기능 나열

이제 서버에 연결하여 기능을 요청합니다:

#### Python

```python
# 사용 가능한 리소스 목록
resources = await session.list_resources()
print("LISTING RESOURCES")
for resource in resources:
    print("Resource: ", resource)

# 사용 가능한 도구 목록
tools = await session.list_tools()
print("LISTING TOOLS")
for tool in tools.tools:
    print("Tool: ", tool.name)
    print("Tool", tool.inputSchema["properties"])
```

추가한 내용은:

- 리소스와 도구를 나열하고 출력했습니다. 도구의 경우 나중에 사용할 `inputSchema`도 나열했습니다. 

위 코드에서 우리는:

- MCP 서버에서 사용 가능한 도구를 나열했습니다.
- 각 도구에 대해 이름, 설명 및 스키마를 나열했습니다. 후자는 곧 도구 호출에 사용할 것입니다.

### -3- 서버 기능을 LLM 도구로 변환

서버 기능을 나열한 다음 단계는 이를 LLM이 이해하는 형식으로 변환하는 것입니다. 이렇게 하면 LLM에 도구로 제공할 수 있습니다.

#### Python

1. 먼저 다음 변환 함수부터 만듭니다:

    ```python
    def convert_to_llm_tool(tool):
        tool_schema = {
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "type": "function",
                "parameters": {
                    "type": "object",
                    "properties": tool.inputSchema["properties"]
                }
            }
        }

        return tool_schema
    ```

    위 함수 `convert_to_llm_tools`는 MCP 도구 응답을 받아 LLM이 이해할 수 있는 형식으로 변환합니다.

2. 다음으로 클라이언트 코드를 업데이트하여 이 함수를 활용합니다:

    ```python
    functions = []
    for tool in tools.tools:
        print("Tool: ", tool.name)
        print("Tool", tool.inputSchema["properties"])
        functions.append(convert_to_llm_tool(tool))
    ```

    여기서는 MCP 도구 응답을 LLM에 전달할 수 있는 형식으로 변환하기 위해 `convert_to_llm_tool` 호출을 추가했습니다.
 
이제 사용자 요청을 처리할 준비가 되었으니 다음으로 넘어갑시다.

### -4- 사용자 프롬프트 요청 처리

이 부분에서는 사용자 요청을 처리합니다.

#### Python

1. LLM 호출에 필요한 일부 임포트를 추가합니다:

    ```python
    # 대형 언어 모델
    import os
    from azure.ai.inference import ChatCompletionsClient
    from azure.ai.inference.models import SystemMessage, UserMessage
    from azure.core.credentials import AzureKeyCredential
    import json
    ```

2. 다음으로 LLM을 호출하는 함수를 추가합니다:

    ```python
    # llm

    def call_llm(prompt, functions):
        token = os.environ["GITHUB_TOKEN"]
        endpoint = "https://models.inference.ai.azure.com"

        model_name = "gpt-4o"

        client = ChatCompletionsClient(
            endpoint=endpoint,
            credential=AzureKeyCredential(token),
        )

        print("CALLING LLM")
        response = client.complete(
            messages=[
                {
                "role": "system",
                "content": "You are a helpful assistant.",
                },
                {
                "role": "user",
                "content": prompt,
                },
            ],
            model=model_name,
            tools = functions,
            # 선택적 매개변수
            temperature=1.,
            max_tokens=1000,
            top_p=1.    
        )

        response_message = response.choices[0].message
        
        functions_to_call = []

        if response_message.tool_calls:
            for tool_call in response_message.tool_calls:
                print("TOOL: ", tool_call)
                name = tool_call.function.name
                args = json.loads(tool_call.function.arguments)
                functions_to_call.append({ "name": name, "args": args })

        return functions_to_call
    ```

    위 코드에서 우리는:

    - MCP 서버에서 찾고 변환한 함수를 LLM에 전달했습니다.
    - 해당 함수를 사용해 LLM을 호출했습니다.
    - 결과를 검사하여 호출해야 할 함수가 있는지 확인했습니다.
    - 호출할 함수 배열을 전달했습니다.

3. 마지막 단계로 메인 코드를 업데이트합니다:

    ```python
    prompt = "Add 2 to 20"

    # 모든 도구가 있다면 LLM에 어떤 도구인지 물어보세요
    functions_to_call = call_llm(prompt, functions)

    # 제안된 함수를 호출하세요
    for f in functions_to_call:
        result = await session.call_tool(f["name"], arguments=f["args"])
        print("TOOLS result: ", result.content)
    ```

    위 코드가 마지막 단계입니다. 여기서는:

    - LLM이 호출해야 한다고 판단한 함수를 사용해 `call_tool`로 MCP 도구를 호출합니다.
    - 도구 호출 결과를 MCP 서버에 출력합니다.

## 과제

연습에서 작성한 코드를 가져와 서버에 더 많은 도구를 추가하여 확장하세요. 그런 다음 연습과 같이 LLM을 사용하는 클라이언트를 만들고 다양한 프롬프트로 테스트하여 모든 서버 도구가 동적으로 호출되는지 확인하세요. 이렇게 클라이언트를 구축하면 최종 사용자가 정확한 클라이언트 명령 대신 프롬프트를 사용하여 MCP 서버 호출을 인지하지 못한 채 훌륭한 사용자 경험을 누릴 수 있습니다.

## 솔루션

[Solution](</3. 시작/3-3. LLM-클라이언트/solution/README.md>)

## 주요 내용

- 클라이언트에 LLM을 추가하면 사용자가 MCP 서버와 상호작용하는 더 나은 방법을 제공합니다.
- MCP 서버 응답을 LLM이 이해할 수 있는 형태로 변환해야 합니다.

## 추가 자료

## 다음 단계

- 다음: [GitHub Copilot 에이전트 모드에서 MCP 서버 사용하기](<../3-4. VS Code/3-4.md>)